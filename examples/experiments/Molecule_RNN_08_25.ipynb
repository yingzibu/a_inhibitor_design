{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPt1GeEZNg7u8Yt4ZWK400X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yingzibu/a_inhibitor_design/blob/main/examples/experiments/Molecule_RNN_08_25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "g4u3cyudcgJJ"
      },
      "outputs": [],
      "source": [
        "! git clone https://github.com/shiwentao00/Molecule-RNN.git --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install rdkit --quiet\n",
        "! pip install selfies --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMG_Daqycvj1",
        "outputId": "3dd3dcfb-2abf-41d4-c5d7-b1696d3d6d6e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.7/29.7 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd Molecule-RNN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEWmbDu5ctAV",
        "outputId": "2095a27e-5fcd-4d4a-b104-3b330fb9cbdd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Molecule-RNN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import yaml\n",
        "import selfies as sf\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ],
      "metadata": {
        "id": "bjpWBtB_czVN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selfie_vocab_path = '/content/Molecule-RNN/vocab/selfies_merged_vocab.yaml'\n",
        "\n",
        "class SELFIEVocab:\n",
        "    def __init__(self, vocab_path):\n",
        "        self.name ='selfies'\n",
        "        with open(vocab_path, 'r') as f: self.vocab = yaml.full_load(f)\n",
        "        self.int2token = {value:key for key, value in self.vocab.items()}\n",
        "    def tokenize_smiles(self, mol): #mol is selfie actually\n",
        "        ints = [self.vocab['<sos>']]\n",
        "        selfies_list = list(sf.split_selfies(mol))\n",
        "        ints += [self.vocab[token] for token in selfies_list]\n",
        "        ints.append(self.vocab['<eos>'])\n",
        "        return ints\n",
        "    def combine_list(self, selfies): return \"\".join(selfies)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "class SMILESDataset(Dataset):\n",
        "    def __init__(self, smiles_file, percentage, vocab):\n",
        "        super(SMILESDataset, self).__init__()\n",
        "        assert (0<percentage<=1)\n",
        "        self.percentage = percentage\n",
        "        self.vocab = vocab\n",
        "        if smiles_file.split('.')[-1] == 'smi':\n",
        "            self.data = self.read_smiles_file(smiles_file)\n",
        "        elif smiles_file.split('.')[-1] == 'csv':\n",
        "            self.data = self.read_csv_file(smiles_file)\n",
        "        print('total number of SMILES loaded: ', len(self.data))\n",
        "\n",
        "        if self.vocab.name == 'selfies':\n",
        "            self.data = self.smi2sf()\n",
        "            print('total valid SELFIES:', len(self.data))\n",
        "    def __getitem__(self, index):\n",
        "        s_mol = self.data[index]\n",
        "        vec_mol = self.vocab.tokenize_smiles(s_mol)\n",
        "        return vec_mol\n",
        "\n",
        "    def __len__(self): return len(self.data)\n",
        "\n",
        "    def smi2sf(self):\n",
        "        smiles = self.data\n",
        "        sfs = []\n",
        "        for i in tqdm(smiles, total=len(smiles)):\n",
        "            try:\n",
        "                s_ = sf.encoder(i)\n",
        "                if s_ is not None: sfs.append(s_)\n",
        "            except: pass\n",
        "        return sfs\n",
        "\n",
        "\n",
        "\n",
        "    def read_smiles_file(self, path):\n",
        "        with open(path, 'r') as f:\n",
        "            smiles = [line.strip(\"\\n\") for line in f.readlines()]\n",
        "        num_data = len(smiles)\n",
        "        return smiles[0:int(num_data*self.percentage)]\n",
        "\n",
        "    def read_csv_file(self, path):\n",
        "        data = pd.read_csv(path)\n",
        "        for i in ['Smiles', 'smiles', 'SMILES']:\n",
        "            if i in data.columns:\n",
        "                smiles = data[i].tolist()\n",
        "                num_data = len(smiles)\n",
        "                return smiles[0:int(num_data*self.percentage)]\n",
        "\n",
        "smile_dir = '/content/Molecule-RNN/dataset/chembl28-cleaned.smi'\n",
        "selfie_vocab_path = '/content/Molecule-RNN/vocab/selfies_merged_vocab.yaml'\n",
        "\n",
        "def dataloader_gen(percentage, batch_size, PADDING_IDX, shuffle,\n",
        "                   dataset_dir=smile_dir, which_vocab='selfies',\n",
        "                   vocab_path=selfie_vocab_path, drop_last=True):\n",
        "    print('which_vocab:', which_vocab)\n",
        "    if which_vocab == 'selfies':\n",
        "        vocab = SELFIEVocab(vocab_path)\n",
        "        dataset = SMILESDataset(dataset_dir, percentage, vocab)\n",
        "    else:\n",
        "        dataset = None\n",
        "        vocab = None\n",
        "\n",
        "    def pad_collate(batch):\n",
        "        lengths = [len(x) for x in batch]\n",
        "        batch = [torch.tensor(x, dtype=torch.long) for x in batch]\n",
        "        x_padded = pad_sequence(batch, batch_first=True,\n",
        "                                padding_value=PADDING_IDX)\n",
        "        return x_padded, lengths\n",
        "\n",
        "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
        "                            shuffle=shuffle, drop_last=drop_last,\n",
        "                            collate_fn=pad_collate)\n",
        "    return dataloader, len(dataset)"
      ],
      "metadata": {
        "id": "_v4i6WvNc9el"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_config = {\n",
        "    'num_embeddings': 78,\n",
        "    'embedding_dim': 256,\n",
        "    'rnn_type': \"GRU\",\n",
        "\n",
        "    \"input_size\": 256,\n",
        "    'hidden_size': 512,\n",
        "    'num_layers': 3,\n",
        "    'dropout': 0,\n",
        "\n",
        "    \"batch_size\": 512,\n",
        "    'shuffle': True,\n",
        "    'num_epoch': 10,\n",
        "    'which_optimizer': 'adam',\n",
        "    'learning_rate': 0.001,\n",
        "    'weight_decay': 1.0e-4\n",
        "}\n"
      ],
      "metadata": {
        "id": "GDjOrzQ_litI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from torch.nn.functional import softmax\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, rnn_config):\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        self.emb = nn.Embedding(num_embeddings = rnn_config['num_embeddings'],\n",
        "                                embedding_dim = rnn_config['embedding_dim'],\n",
        "                                padding_idx = rnn_config['num_embeddings'] - 1)\n",
        "\n",
        "        if rnn_config['rnn_type'] == 'LSTM':\n",
        "            self.rnn = nn.LSTM(input_size = rnn_config['input_size'],\n",
        "                               hidden_size = rnn_config['hidden_size'],\n",
        "                               num_layers = rnn_config['num_layers'],\n",
        "                               batch_first=True, dropout=rnn_config['dropout'])\n",
        "        elif rnn_config['rnn_type'] == 'GRU':\n",
        "            self.rnn = nn.GRU(input_size = rnn_config['input_size'],\n",
        "                            hidden_size = rnn_config['hidden_size'],\n",
        "                            num_layers = rnn_config['num_layers'],\n",
        "                            batch_first=True, dropout=rnn_config['dropout'])\n",
        "        else: raise ValueError('rnn_type should be LSTM or GRU')\n",
        "\n",
        "        self.linear = nn.Linear(rnn_config['hidden_size'],\n",
        "                                rnn_config['num_embeddings']-2)\n",
        "\n",
        "    def forward(self, data, lengths):\n",
        "        # print('data shape: ', data.shape) # [batch_size, ...]\n",
        "        embeddings = self.emb(data)\n",
        "        # print('emb shape:', embeddings.shape) #[batch_size, ..., embedding_dim]\n",
        "        embeddings = pack_padded_sequence(input=embeddings, lengths=lengths,\n",
        "                                    batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        embeddings, _ = self.rnn(embeddings)\n",
        "        embeddings = self.linear(embeddings.data)\n",
        "\n",
        "        # print('here emb shpae:', embeddings.shape) # [batch_size*?, num_embedding-2]\n",
        "        return embeddings\n",
        "\n",
        "    def sample(self, batch_size, vocab, max_length=140):\n",
        "        start_int = vocab.vocab['<sos>']\n",
        "        if torch.cuda.is_available(): device='cuda'\n",
        "        else: device = 'cpu'\n",
        "        sos = torch.ones([batch_size, 1], dtype=torch.long, device=device)\n",
        "        sos = sos * start_int\n",
        "        output = []\n",
        "        x = self.emb(sos)\n",
        "        x, hidden = self.rnn(x)\n",
        "        x = self.linear(x)\n",
        "        x = softmax(x, dim=-1)\n",
        "        x = torch.multinomial(x.squeeze(), 1)\n",
        "        output.append(x)\n",
        "        # x = sos\n",
        "        finish = torch.zeros(batch_size, dtype=torch.bool).to(device)\n",
        "        for _ in range(max_length):\n",
        "            x = self.emb(x)\n",
        "            x, hidden = self.rnn(x, hidden)\n",
        "            x = self.linear(x)\n",
        "            x = softmax(x, dim=-1)\n",
        "            x = torch.multinomial(x.squeeze(), 1)\n",
        "            output.append(x)\n",
        "            eos_sampled = (x==vocab.vocab['<eos>']).data\n",
        "            finish = torch.logical_or(finish, eos_sampled.squeeze())\n",
        "            if torch.all(finish):\n",
        "                return torch.cat(output, -1)\n",
        "        return torch.cat(output, -1)"
      ],
      "metadata": {
        "id": "PltcR1YdlMPe"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from rdkit import Chem\n",
        "import selfies as sf\n",
        "from rdkit import rdBase\n",
        "rdBase.DisableLog('rdApp.error')\n",
        "\n",
        "def sample(model, vocab, batch_size):\n",
        "    model.eval()\n",
        "    sample_ints = model.sample(batch_size=batch_size, vocab=vocab).tolist()\n",
        "    molecules = []\n",
        "    for ints in sample_ints:\n",
        "        mol = []\n",
        "        for x in ints:\n",
        "            if vocab.int2token[x] == '<eos>': break\n",
        "            else: mol.append(vocab.int2token[x])\n",
        "        string_mol = \"\".join(mol)\n",
        "        molecules.append(string_mol)\n",
        "    if vocab.name == 'selfies':\n",
        "        molecules = [sf.decoder(x) for x in molecules]\n",
        "    return molecules\n",
        "\n",
        "\n",
        "def compute_valid_rate(molecules):\n",
        "    num_valid, num_invalid = 0, 0\n",
        "    valid_mols = set()\n",
        "    for i in molecules:\n",
        "        mol = Chem.MolFromSmiles(i)\n",
        "        if mol is None: num_invalid +=1\n",
        "        else:\n",
        "            num_valid += 1\n",
        "            valid_mols.add(i)\n",
        "    assert len(molecules) == num_valid + num_invalid\n",
        "    unique_num = len(valid_mols)\n",
        "    print(unique_num)\n",
        "    valid_rate = num_valid / len(molecules)\n",
        "    unique_rate = unique_num / len(molecules)\n",
        "    return valid_rate, unique_rate\n"
      ],
      "metadata": {
        "id": "7ut-atLHsJMD"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cuda = torch.cuda.is_available()\n",
        "if cuda: device = 'cuda'\n",
        "else: device = 'cpu'\n",
        "\n",
        "out_dir = 'model_save/'\n",
        "if not os.path.exists(out_dir): os.makedirs(out_dir)\n",
        "\n",
        "with open(out_dir+'config.yaml', 'w') as f: yaml.dump(rnn_config, f)"
      ],
      "metadata": {
        "id": "NJtP2IBzt_E-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smile_dir = '/content/Molecule-RNN/dataset/chembl28-cleaned.smi'\n",
        "selfie_vocab_path = '/content/Molecule-RNN/vocab/selfies_merged_vocab.yaml'"
      ],
      "metadata": {
        "id": "I22zlpsnu7Ef"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "SELFIES: a robust representation of semantically constrained graphs with an\n",
        "    example application in chemistry (https://arxiv.org/abs/1905.13741)\n",
        "    by Mario Krenn, Florian Haese, AkshatKuman Nigam, Pascal Friederich,\n",
        "    Alan Aspuru-Guzik.\n",
        "\n",
        "    Variational Autoencoder (VAE) for chemistry\n",
        "        comparing SMILES and SELFIES representation using reconstruction\n",
        "        quality, diversity and latent space validity as metrics of\n",
        "        interest\n",
        "\n",
        "information:\n",
        "    ML framework: pytorch\n",
        "    chemistry framework: RDKit\n",
        "\n",
        "    get_selfie_and_smiles_encodings_for_dataset\n",
        "        generate complete encoding (inclusive alphabet) for SMILES and\n",
        "        SELFIES given a data file\n",
        "\n",
        "    VAEEncoder\n",
        "        fully connected, 3 layer neural network - encodes a one-hot\n",
        "        representation of molecule (in SMILES or SELFIES representation)\n",
        "        to latent space\n",
        "\n",
        "    VAEDecoder\n",
        "        decodes point in latent space using an RNN\n",
        "\n",
        "    latent_space_quality\n",
        "        samples points from latent space, decodes them into molecules,\n",
        "        calculates chemical validity (using RDKit's MolFromSmiles), calculates\n",
        "        diversity\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import yaml\n",
        "from rdkit import rdBase\n",
        "from rdkit.Chem import MolFromSmiles\n",
        "from torch import nn\n",
        "import selfies as sf\n",
        "\n",
        "\n",
        "# from data_loader import \\\n",
        "#     multiple_selfies_to_hot, multiple_smile_to_hot\n",
        "\n",
        "rdBase.DisableLog('rdApp.error')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\"\"\"\n",
        "This file is to encode SMILES and SELFIES into one-hot encodings\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from os import walk\n",
        "import torch\n",
        "from rdkit.Chem.Draw import IPythonConsole, MolsToGridImage\n",
        "from rdkit import Chem\n",
        "m = Chem.MolFromSmiles\n",
        "import math\n",
        "\n",
        "\n",
        "def smile_to_hot(smile, largest_smile_len, alphabet):\n",
        "    \"\"\"Go from a single smile string to a one-hot encoding.\n",
        "    \"\"\"\n",
        "\n",
        "    char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
        "\n",
        "    # pad with ' '\n",
        "    smile += ' ' * (largest_smile_len - len(smile))\n",
        "\n",
        "    # integer encode input smile\n",
        "    integer_encoded = [char_to_int[char] for char in smile]\n",
        "\n",
        "    # one hot-encode input smile\n",
        "    onehot_encoded = list()\n",
        "    for value in integer_encoded:\n",
        "        letter = [0 for _ in range(len(alphabet))]\n",
        "        letter[value] = 1\n",
        "        onehot_encoded.append(letter)\n",
        "    return integer_encoded, np.array(onehot_encoded)\n",
        "\n",
        "\n",
        "def multiple_smile_to_hot(smiles_list, largest_molecule_len, alphabet):\n",
        "    \"\"\"Convert a list of smile strings to a one-hot encoding\n",
        "\n",
        "    Returned shape (num_smiles x len_of_largest_smile x len_smile_encoding)\n",
        "    \"\"\"\n",
        "\n",
        "    hot_list = []\n",
        "    for s in smiles_list:\n",
        "        _, onehot_encoded = smile_to_hot(s, largest_molecule_len, alphabet)\n",
        "        hot_list.append(onehot_encoded)\n",
        "    return np.array(hot_list)\n",
        "\n",
        "\n",
        "def selfies_to_hot(selfie, largest_selfie_len, alphabet):\n",
        "    \"\"\"Go from a single selfies string to a one-hot encoding.\n",
        "    \"\"\"\n",
        "\n",
        "    symbol_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
        "\n",
        "    # pad with [nop]\n",
        "    selfie += '[nop]' * (largest_selfie_len - sf.len_selfies(selfie))\n",
        "\n",
        "    # integer encode\n",
        "    symbol_list = sf.split_selfies(selfie)\n",
        "    integer_encoded = [symbol_to_int[symbol] for symbol in symbol_list]\n",
        "\n",
        "    # one hot-encode the integer encoded selfie\n",
        "    onehot_encoded = list()\n",
        "    for index in integer_encoded:\n",
        "        letter = [0] * len(alphabet)\n",
        "        letter[index] = 1\n",
        "        onehot_encoded.append(letter)\n",
        "\n",
        "    return integer_encoded, np.array(onehot_encoded)\n",
        "\n",
        "\n",
        "def multiple_selfies_to_hot(selfies_list, largest_molecule_len, alphabet):\n",
        "    \"\"\"Convert a list of selfies strings to a one-hot encoding\n",
        "    \"\"\"\n",
        "\n",
        "    hot_list = []\n",
        "    for s in selfies_list:\n",
        "        _, onehot_encoded = selfies_to_hot(s, largest_molecule_len, alphabet)\n",
        "        hot_list.append(onehot_encoded)\n",
        "    return np.array(hot_list)\n",
        "\n",
        "def _make_dir(directory):\n",
        "    os.makedirs(directory)\n",
        "\n",
        "\n",
        "def save_models(encoder, decoder, epoch):\n",
        "    out_dir = './saved_models/{}'.format(epoch)\n",
        "    _make_dir(out_dir)\n",
        "    torch.save(encoder, '{}/E'.format(out_dir))\n",
        "    torch.save(decoder, '{}/D'.format(out_dir))\n",
        "\n",
        "\n",
        "class VAEEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self, in_dimension, layer_1d, layer_2d, layer_3d,\n",
        "                 latent_dimension):\n",
        "        \"\"\"\n",
        "        Fully Connected layers to encode molecule to latent space\n",
        "        \"\"\"\n",
        "        super(VAEEncoder, self).__init__()\n",
        "        self.latent_dimension = latent_dimension\n",
        "\n",
        "        # Reduce dimension up to second last layer of Encoder\n",
        "        self.encode_nn = nn.Sequential(\n",
        "            nn.Linear(in_dimension, layer_1d),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(layer_1d, layer_2d),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(layer_2d, layer_3d),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Latent space mean\n",
        "        self.encode_mu = nn.Linear(layer_3d, latent_dimension)\n",
        "\n",
        "        # Latent space variance\n",
        "        self.encode_log_var = nn.Linear(layer_3d, latent_dimension)\n",
        "\n",
        "    @staticmethod\n",
        "    def reparameterize(mu, log_var):\n",
        "        \"\"\"\n",
        "        This trick is explained well here:\n",
        "            https://stats.stackexchange.com/a/16338\n",
        "        \"\"\"\n",
        "        std = torch.exp(0.5 * log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        return eps.mul(std).add_(mu)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Pass throught the Encoder\n",
        "        \"\"\"\n",
        "        # Get results of encoder network\n",
        "        h1 = self.encode_nn(x)\n",
        "\n",
        "        # latent space\n",
        "        mu = self.encode_mu(h1)\n",
        "        log_var = self.encode_log_var(h1)\n",
        "\n",
        "        # Reparameterize\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "        return z, mu, log_var\n",
        "\n",
        "\n",
        "class VAEDecoder(nn.Module):\n",
        "\n",
        "    def __init__(self, latent_dimension, gru_stack_size, gru_neurons_num,\n",
        "                 out_dimension):\n",
        "        \"\"\"\n",
        "        Through Decoder\n",
        "        \"\"\"\n",
        "        super(VAEDecoder, self).__init__()\n",
        "        self.latent_dimension = latent_dimension\n",
        "        self.gru_stack_size = gru_stack_size\n",
        "        self.gru_neurons_num = gru_neurons_num\n",
        "\n",
        "        # Simple Decoder\n",
        "        self.decode_RNN = nn.GRU(\n",
        "            input_size=latent_dimension,\n",
        "            hidden_size=gru_neurons_num,\n",
        "            num_layers=gru_stack_size,\n",
        "            batch_first=False)\n",
        "\n",
        "        self.decode_FC = nn.Sequential(\n",
        "            nn.Linear(gru_neurons_num, out_dimension),\n",
        "        )\n",
        "\n",
        "    def init_hidden(self, batch_size=1):\n",
        "        weight = next(self.parameters())\n",
        "        return weight.new_zeros(self.gru_stack_size, batch_size,\n",
        "                                self.gru_neurons_num)\n",
        "\n",
        "    def forward(self, z, hidden):\n",
        "        \"\"\"\n",
        "        A forward pass throught the entire model.\n",
        "        \"\"\"\n",
        "\n",
        "        # Decode\n",
        "        l1, hidden = self.decode_RNN(z, hidden)\n",
        "        decoded = self.decode_FC(l1)  # fully connected layer\n",
        "\n",
        "        return decoded, hidden\n",
        "\n",
        "\n",
        "def is_correct_smiles(smiles):\n",
        "    \"\"\"\n",
        "    Using RDKit to calculate whether molecule is syntactically and\n",
        "    semantically valid.\n",
        "    \"\"\"\n",
        "    if smiles == \"\":\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        return MolFromSmiles(smiles, sanitize=True) is not None\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "\n",
        "def sample_latent_space(vae_encoder, vae_decoder, sample_len):\n",
        "    vae_encoder.eval()\n",
        "    vae_decoder.eval()\n",
        "\n",
        "    gathered_atoms = []\n",
        "\n",
        "    fancy_latent_point = torch.randn(1, 1, vae_encoder.latent_dimension,\n",
        "                                     device=device)\n",
        "    hidden = vae_decoder.init_hidden()\n",
        "\n",
        "    # runs over letters from molecules (len=size of largest molecule)\n",
        "    for _ in range(sample_len):\n",
        "        out_one_hot, hidden = vae_decoder(fancy_latent_point, hidden)\n",
        "\n",
        "        out_one_hot = out_one_hot.flatten().detach()\n",
        "        soft = nn.Softmax(0)\n",
        "        out_one_hot = soft(out_one_hot)\n",
        "\n",
        "        out_index = out_one_hot.argmax(0)\n",
        "        gathered_atoms.append(out_index.data.cpu().tolist())\n",
        "\n",
        "    vae_encoder.train()\n",
        "    vae_decoder.train()\n",
        "\n",
        "    return gathered_atoms\n",
        "\n",
        "import random\n",
        "import math\n",
        "from rdkit.Chem.Draw import IPythonConsole, MolsToGridImage\n",
        "from rdkit import Chem\n",
        "mol_conv = Chem.MolFromSmiles\n",
        "\n",
        "def latent_space_quality(vae_encoder, vae_decoder, type_of_encoding,\n",
        "                         alphabet, sample_num, sample_len, epoch):\n",
        "    total_correct = 0\n",
        "    all_correct_molecules = set()\n",
        "    print(f\"latent_space_quality:\"\n",
        "          f\" Take {sample_num} samples from the latent space\")\n",
        "\n",
        "    for _ in tqdm(range(1, sample_num + 1), total=sample_num):\n",
        "\n",
        "        molecule_pre = ''\n",
        "        for i in sample_latent_space(vae_encoder, vae_decoder, sample_len):\n",
        "            molecule_pre += alphabet[i]\n",
        "        molecule = molecule_pre.replace(' ', '')\n",
        "\n",
        "        if type_of_encoding == 1:  # if SELFIES, decode to SMILES\n",
        "            molecule = sf.decoder(molecule)\n",
        "\n",
        "        if is_correct_smiles(molecule):\n",
        "            total_correct += 1\n",
        "            all_correct_molecules.add(molecule)\n",
        "\n",
        "    MAX_GRID = 16\n",
        "    show_num = min(len(all_correct_molecules), MAX_GRID)\n",
        "    selected_smiles = random.choices([*all_correct_molecules], k=show_num)\n",
        "    molecules = [mol_conv(i) for i in selected_smiles]\n",
        "\n",
        "    img = MolsToGridImage(\n",
        "        [m for m in molecules if m is not None],\n",
        "        molsPerRow=int(math.sqrt(MAX_GRID)),\n",
        "        subImgSize=(250, 150),  returnPNG=False\n",
        "        # legends=legends,\n",
        "    )\n",
        "    display(img)\n",
        "    import os\n",
        "    if not os.path.exists('figs/'): os.makedirs('figs/')\n",
        "    img.save(f'figs/latent_sample_{epoch}.png')\n",
        "    print('figure saved at :', 'figs/', f'latent_sample_{epoch}.png')\n",
        "\n",
        "    return total_correct, len(all_correct_molecules)\n",
        "\n",
        "\n",
        "def quality_in_valid_set(vae_encoder, vae_decoder, data_valid, batch_size):\n",
        "    data_valid = data_valid[torch.randperm(data_valid.size()[0])]  # shuffle\n",
        "    num_batches_valid = len(data_valid) // batch_size\n",
        "\n",
        "    quality_list = []\n",
        "    for batch_iteration in range(min(25, num_batches_valid)):\n",
        "\n",
        "        # get batch\n",
        "        start_idx = batch_iteration * batch_size\n",
        "        stop_idx = (batch_iteration + 1) * batch_size\n",
        "        batch = data_valid[start_idx: stop_idx]\n",
        "        _, trg_len, _ = batch.size()\n",
        "\n",
        "        inp_flat_one_hot = batch.flatten(start_dim=1)\n",
        "        latent_points, mus, log_vars = vae_encoder(inp_flat_one_hot)\n",
        "\n",
        "        latent_points = latent_points.unsqueeze(0)\n",
        "        hidden = vae_decoder.init_hidden(batch_size=batch_size)\n",
        "        out_one_hot = torch.zeros_like(batch, device=device)\n",
        "        for seq_index in range(trg_len):\n",
        "            out_one_hot_line, hidden = vae_decoder(latent_points, hidden)\n",
        "            out_one_hot[:, seq_index, :] = out_one_hot_line[0]\n",
        "\n",
        "        # assess reconstruction quality\n",
        "        quality = compute_recon_quality(batch, out_one_hot)\n",
        "        quality_list.append(quality)\n",
        "\n",
        "    return np.mean(quality_list).item()\n",
        "\n",
        "import shutil\n",
        "def train_model(vae_encoder, vae_decoder,\n",
        "                data_train, data_valid, num_epochs, batch_size,\n",
        "                lr_enc, lr_dec, KLD_alpha,\n",
        "                sample_num, sample_len, alphabet, type_of_encoding):\n",
        "    \"\"\"\n",
        "    Train the Variational Auto-Encoder\n",
        "    \"\"\"\n",
        "\n",
        "    print('num_epochs: ', num_epochs)\n",
        "\n",
        "    # initialize an instance of the model\n",
        "    optimizer_encoder = torch.optim.Adam(vae_encoder.parameters(), lr=lr_enc)\n",
        "    optimizer_decoder = torch.optim.Adam(vae_decoder.parameters(), lr=lr_dec)\n",
        "\n",
        "    data_train = data_train.clone().detach().to(device)\n",
        "    num_batches_train = int(len(data_train) / batch_size)\n",
        "\n",
        "    quality_valid_list = [0, 0, 0, 0]\n",
        "    folder_names = []\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        data_train = data_train[torch.randperm(data_train.size()[0])]\n",
        "\n",
        "        start = time.time()\n",
        "        for batch_iteration in range(num_batches_train):  # batch iterator\n",
        "\n",
        "            # manual batch iterations\n",
        "            start_idx = batch_iteration * batch_size\n",
        "            stop_idx = (batch_iteration + 1) * batch_size\n",
        "            batch = data_train[start_idx: stop_idx]\n",
        "\n",
        "            # reshaping for efficient parallelization\n",
        "            inp_flat_one_hot = batch.flatten(start_dim=1)\n",
        "            latent_points, mus, log_vars = vae_encoder(inp_flat_one_hot)\n",
        "\n",
        "            # initialization hidden internal state of RNN (RNN has two inputs\n",
        "            # and two outputs:)\n",
        "            #    input: latent space & hidden state\n",
        "            #    output: one-hot encoding of one character of molecule & hidden\n",
        "            #    state the hidden state acts as the internal memory\n",
        "            latent_points = latent_points.unsqueeze(0)\n",
        "            hidden = vae_decoder.init_hidden(batch_size=batch_size)\n",
        "\n",
        "            # decoding from RNN N times, where N is the length of the largest\n",
        "            # molecule (all molecules are padded)\n",
        "            out_one_hot = torch.zeros_like(batch, device=device)\n",
        "            for seq_index in range(batch.shape[1]):\n",
        "                out_one_hot_line, hidden = vae_decoder(latent_points, hidden)\n",
        "                out_one_hot[:, seq_index, :] = out_one_hot_line[0]\n",
        "\n",
        "            # compute ELBO\n",
        "            loss = compute_elbo(batch, out_one_hot, mus, log_vars, KLD_alpha)\n",
        "\n",
        "            # perform back propogation\n",
        "            optimizer_encoder.zero_grad()\n",
        "            optimizer_decoder.zero_grad()\n",
        "            loss.backward(retain_graph=True)\n",
        "            nn.utils.clip_grad_norm_(vae_decoder.parameters(), 0.5)\n",
        "            optimizer_encoder.step()\n",
        "            optimizer_decoder.step()\n",
        "\n",
        "            if (batch_iteration+1) % int(num_batches_train/3) == 0:\n",
        "                end = time.time()\n",
        "\n",
        "                # assess reconstruction quality\n",
        "                quality_train = compute_recon_quality(batch, out_one_hot)\n",
        "                quality_valid = quality_in_valid_set(vae_encoder, vae_decoder,\n",
        "                                                     data_valid, batch_size)\n",
        "\n",
        "                report = 'Epoch: %d,  Batch: %d / %d, (loss: %.3f | ' \\\n",
        "                         'quality: %.3f | quality_valid: %.3f)   ' \\\n",
        "                         'ELAPSED TIME: %.5f' \\\n",
        "                         % (epoch, batch_iteration, num_batches_train,\n",
        "                            loss.item(), quality_train, quality_valid,\n",
        "                            end - start)\n",
        "                print(report)\n",
        "                start = time.time()\n",
        "\n",
        "        quality_valid = quality_in_valid_set(vae_encoder, vae_decoder,\n",
        "                                             data_valid, batch_size)\n",
        "        quality_valid_list.append(quality_valid)\n",
        "\n",
        "        # only measure validity of reconstruction improved\n",
        "        quality_increase = len(quality_valid_list) \\\n",
        "                           - np.argmax(quality_valid_list)\n",
        "        if quality_increase == 1 and quality_valid_list[-1] > 50.:\n",
        "            corr, unique = latent_space_quality(vae_encoder, vae_decoder,\n",
        "                                                type_of_encoding, alphabet,\n",
        "                                                sample_num, sample_len, epoch)\n",
        "\n",
        "            model_path = f'vae/epoch_{epoch}/'\n",
        "            if not os.path.exists(model_path): os.makedirs(model_path)\n",
        "            folder_names.append(model_path)\n",
        "            torch.save(vae_encoder.state_dict(), model_path+'enc.pt')\n",
        "            torch.save(vae_decoder.state_dict(), model_path+'dec.pt')\n",
        "            while len(folder_names) > 10:\n",
        "                try:\n",
        "                    remove_dir_name = folder_names.pop(0)\n",
        "                    print('remove dir: ', remove_dir_name)\n",
        "                    # os.rmdir(remove_dir_name) # cannot delete non empty dir\n",
        "\n",
        "                    shutil.rmtree(remove_dir_name, ignore_errors=True)\n",
        "                except:\n",
        "                    print(f'error when delete {folder_names[0]}')\n",
        "        else:\n",
        "            corr, unique = -1., -1.\n",
        "\n",
        "        report = 'Validity: %.3f %% | Diversity: %.3f %% | ' \\\n",
        "                 'Reconstruction: %.3f %%' \\\n",
        "                 % (corr * 100. / sample_num, unique * 100. / sample_num,\n",
        "                    quality_valid)\n",
        "        print(report)\n",
        "\n",
        "        with open('results.dat', 'a') as content:\n",
        "            content.write(report + '\\n')\n",
        "\n",
        "        if quality_valid_list[-1] < 70. and epoch > 200:\n",
        "            break\n",
        "\n",
        "        if quality_increase > 20:\n",
        "            print('Early stopping criteria')\n",
        "            break\n",
        "\n",
        "\n",
        "def compute_elbo(x, x_hat, mus, log_vars, KLD_alpha):\n",
        "    inp = x_hat.reshape(-1, x_hat.shape[2])\n",
        "    target = x.reshape(-1, x.shape[2]).argmax(1)\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    recon_loss = criterion(inp, target)\n",
        "    kld = -0.5 * torch.mean(1. + log_vars - mus.pow(2) - log_vars.exp())\n",
        "\n",
        "    return recon_loss + KLD_alpha * kld\n",
        "\n",
        "\n",
        "def compute_recon_quality(x, x_hat):\n",
        "    x_indices = x.reshape(-1, x.shape[2]).argmax(1)\n",
        "    x_hat_indices = x_hat.reshape(-1, x_hat.shape[2]).argmax(1)\n",
        "\n",
        "    differences = 1. - torch.abs(x_hat_indices - x_indices)\n",
        "    differences = torch.clamp(differences, min=0., max=1.).double()\n",
        "    quality = 100. * torch.mean(differences)\n",
        "    quality = quality.detach().cpu().numpy()\n",
        "\n",
        "    return quality\n",
        "\n",
        "\n",
        "\n",
        "def main(file_name_smiles, type_of_encoding, dynamic_z_dim=False):\n",
        "    content = open('logfile.dat', 'w')\n",
        "    content.close()\n",
        "    content = open('results.dat', 'w')\n",
        "    content.close()\n",
        "\n",
        "    if os.path.exists(\"settings.yml\"):\n",
        "        settings = yaml.safe_load(open(\"settings.yml\", \"r\"))\n",
        "    else:\n",
        "        print(\"Expected a file settings.yml but didn't find it.\")\n",
        "        return\n",
        "\n",
        "    print('--> Acquiring data...')\n",
        "    # type_of_encoding = settings['data']['type_of_encoding']\n",
        "    # file_name_smiles = settings['data']['smiles_file']\n",
        "\n",
        "    print('Finished acquiring data.')\n",
        "\n",
        "    if type_of_encoding == 0:\n",
        "        print('Representation: SMILES')\n",
        "        print('There are :', len(file_name_smiles), ' compounds')\n",
        "        _, _, _, encoding_list, encoding_alphabet, largest_molecule_len = \\\n",
        "            get_selfie_and_smiles_encodings_for_dataset(file_name_smiles)\n",
        "\n",
        "        print('--> Creating one-hot encoding...')\n",
        "        data = multiple_smile_to_hot(encoding_list, largest_molecule_len,\n",
        "                                     encoding_alphabet)\n",
        "        print('Finished creating one-hot encoding.')\n",
        "\n",
        "    elif type_of_encoding == 1:\n",
        "        print('Representation: SELFIES')\n",
        "        encoding_list, encoding_alphabet, largest_molecule_len, _, _, _ = \\\n",
        "            get_selfie_and_smiles_encodings_for_dataset(file_name_smiles)\n",
        "\n",
        "        print('--> Creating one-hot encoding...')\n",
        "        data = multiple_selfies_to_hot(encoding_list, largest_molecule_len,\n",
        "                                       encoding_alphabet)\n",
        "        print('Finished creating one-hot encoding.')\n",
        "\n",
        "    else:\n",
        "        print(\"type_of_encoding not in {0, 1}.\")\n",
        "        return\n",
        "    len_compound = data.shape[0]\n",
        "    len_max_molec = data.shape[1]\n",
        "    len_alphabet = data.shape[2]\n",
        "    len_max_mol_one_hot = len_max_molec * len_alphabet\n",
        "\n",
        "    print(' ')\n",
        "    print(f\"Dataset has {len_compound} compounds, \"\n",
        "          f\"Alphabet has {len_alphabet} letters, \"\n",
        "          f\"largest molecule is {len_max_molec} letters.\")\n",
        "\n",
        "    data_parameters = settings['data']\n",
        "    batch_size = data_parameters['batch_size']\n",
        "\n",
        "    if dynamic_z_dim:\n",
        "        z_dim = int(len_max_mol_one_hot/300)*100\n",
        "        settings['encoder']['latent_dimension'] = z_dim\n",
        "        settings['decoder']['latent_dimension'] = z_dim\n",
        "        print(f'z_dim {z_dim} is set to ~ 1/3 of in_dim {len_max_mol_one_hot}')\n",
        "    print(settings['encoder'])\n",
        "    encoder_parameter = settings['encoder']\n",
        "    decoder_parameter = settings['decoder']\n",
        "    training_parameters = settings['training']\n",
        "\n",
        "    vae_encoder = VAEEncoder(in_dimension=len_max_mol_one_hot,\n",
        "                             **encoder_parameter).to(device)\n",
        "    vae_decoder = VAEDecoder(**decoder_parameter,\n",
        "                             out_dimension=len(encoding_alphabet)).to(device)\n",
        "\n",
        "    print('*' * 15, ': -->', device)\n",
        "\n",
        "    data = torch.tensor(data, dtype=torch.float).to(device)\n",
        "\n",
        "    train_valid_test_size = [0.5, 0.5, 0.0]\n",
        "    data = data[torch.randperm(data.size()[0])]\n",
        "    idx_train_val = int(len(data) * train_valid_test_size[0])\n",
        "    idx_val_test = idx_train_val + int(len(data) * train_valid_test_size[1])\n",
        "\n",
        "    data_train = data[0:idx_train_val]\n",
        "    data_valid = data[idx_train_val:idx_val_test]\n",
        "\n",
        "    print(\"start training\")\n",
        "    train_model(**training_parameters,\n",
        "                vae_encoder=vae_encoder,\n",
        "                vae_decoder=vae_decoder,\n",
        "                batch_size=batch_size,\n",
        "                data_train=data_train,\n",
        "                data_valid=data_valid,\n",
        "                alphabet=encoding_alphabet,\n",
        "                type_of_encoding=type_of_encoding,\n",
        "                sample_len=len_max_molec)\n",
        "\n",
        "    with open('COMPLETED', 'w') as content:\n",
        "        content.write('exit code: 0')\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     try:\n",
        "#         main()\n",
        "#     except AttributeError:\n",
        "#         _, error_message, _ = sys.exc_info()\n",
        "#         print(error_message)"
      ],
      "metadata": {
        "id": "8UvSSq08HYFv"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_smiles_file(path):\n",
        "    with open(path, 'r') as f:\n",
        "        smiles = [line.strip(\"\\n\") for line in f.readlines()]\n",
        "    num_data = len(smiles)\n",
        "    return smiles\n",
        "\n",
        "smiles_all = read_smiles_file(smile_dir)\n",
        "len(smiles_all)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PB6IESXsHGOX",
        "outputId": "3d3e3153-1171-4fd8-e1eb-3ab2359d6148"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "538247"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a, b, c, d, e, f = get_selfie_and_smiles_encodings_for_dataset(smiles_all)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drO0yadBHl5h",
        "outputId": "e9dc1178-f452-4c70-9a0d-37938beaa203"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> Translating SMILES to SELFIES...\n",
            "Finished translating 538247 SMILES to SELFIES.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoding_list = a\n",
        "encoding_alphabet = b\n",
        "largest_molecule_len = c\n",
        "data = multiple_selfies_to_hot(encoding_list, largest_molecule_len,\n",
        "                                       encoding_alphabet)\n",
        "len_compound = data.shape[0]\n",
        "len_max_molec = data.shape[1]\n",
        "len_alphabet = data.shape[2]\n",
        "len_max_mol_one_hot = len_max_molec * len_alphabet\n",
        "\n",
        "print(' ')\n",
        "print(f\"Dataset has {len_compound} compounds, \"\n",
        "        f\"Alphabet has {len_alphabet} letters, \"\n",
        "        f\"largest molecule is {len_max_molec} letters.\")"
      ],
      "metadata": {
        "id": "kUbKkIC0H1yC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_selfie_and_smiles_encodings_for_dataset(df):\n",
        "    \"\"\"\n",
        "    Returns encoding, alphabet and length of largest molecule in SMILES and\n",
        "    SELFIES, given a file containing SMILES molecules.\n",
        "\n",
        "    input:\n",
        "        csv file with molecules. Column's name must be 'smiles'.\n",
        "    output:\n",
        "        - selfies encoding\n",
        "        - selfies alphabet\n",
        "        - longest selfies string\n",
        "        - smiles encoding (equivalent to file content)\n",
        "        - smiles alphabet (character based)\n",
        "        - longest smiles string\n",
        "    \"\"\"\n",
        "    # if isinstance(file_path_or_df, str):\n",
        "    #     file_path = file_path_or_df\n",
        "    #     df = pd.read_csv(file_path)\n",
        "    # else: df = file_path_or_df\n",
        "\n",
        "    smiles_list = np.asanyarray(df)\n",
        "\n",
        "    smiles_alphabet = list(set(''.join(smiles_list)))\n",
        "    smiles_alphabet.append(' ')  # for padding\n",
        "\n",
        "    largest_smiles_len = len(max(smiles_list, key=len))\n",
        "\n",
        "    print('--> Translating SMILES to SELFIES...')\n",
        "    selfies_list = list(map(sf.encoder, smiles_list))\n",
        "\n",
        "    all_selfies_symbols = sf.get_alphabet_from_selfies(selfies_list)\n",
        "    all_selfies_symbols.add('[nop]')\n",
        "    selfies_alphabet = list(all_selfies_symbols)\n",
        "\n",
        "    largest_selfies_len = max(sf.len_selfies(s) for s in selfies_list)\n",
        "\n",
        "    print(f'Finished translating {len(selfies_list)} SMILES to SELFIES.')\n",
        "\n",
        "    return selfies_list, selfies_alphabet, largest_selfies_len, \\\n",
        "           smiles_list, smiles_alphabet, largest_smiles_len\n"
      ],
      "metadata": {
        "id": "DF8PPwCcHePV"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(rnn_config)\n",
        "dataset_dir = smile_dir\n",
        "which_vocab = 'selfies'\n",
        "vocab_path = selfie_vocab_path\n",
        "percentage = 0.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbvEM_C9u3Ob",
        "outputId": "ae2eb4e3-dd9d-4877-cbe0-9534e8681e0d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'num_embeddings': 78, 'embedding_dim': 256, 'rnn_type': 'GRU', 'input_size': 256, 'hidden_size': 512, 'num_layers': 3, 'dropout': 0, 'batch_size': 512, 'shuffle': True, 'num_epoch': 10, 'which_optimizer': 'adam', 'learning_rate': 0.001, 'weight_decay': 0.0001}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = rnn_config['batch_size']\n",
        "shuffle = rnn_config['shuffle']\n",
        "PADDING_IDX = rnn_config['num_embeddings']-1\n",
        "num_workers = os.cpu_count()\n",
        "print('number of workers to load data:', num_workers)\n",
        "print('which vocab to use: ', which_vocab)\n",
        "vocab = SELFIEVocab(selfie_vocab_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cA3QdYqvieM",
        "outputId": "c2be79aa-edbb-4cbf-f6c9-d18be1b78e6a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of workers to load data: 2\n",
            "which vocab to use:  selfies\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader, train_size = dataloader_gen(percentage, batch_size, PADDING_IDX,\n",
        "                                        shuffle, drop_last=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2-TJTo26zsD",
        "outputId": "babf19bb-0066-44cd-e2e2-2e4e7980741c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "which_vocab: selfies\n",
            "total number of SMILES loaded:  269123\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 269123/269123 [02:59<00:00, 1501.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total valid SELFIES: 269123\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNN(rnn_config).to(device)\n",
        "learning_rate = rnn_config['learning_rate']\n",
        "weight_decay = rnn_config['weight_decay']\n",
        "loss_function = nn.CrossEntropyLoss(reduction='sum')\n",
        "\n",
        "if rnn_config['which_optimizer'] == 'adam':\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
        "                                 weight_decay=weight_decay, amsgrad=True)\n",
        "elif rnn_config['which_optimizer'] == 'sgd':\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate,\n",
        "                                weight_decay=weight_decay, momentum=0.9)\n",
        "elif rnn_config['which_optimizer'] == 'adamw':\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate,\n",
        "                                 weight_decay=weight_decay, amsgrad=True)\n",
        "else: raise ValueError('wrong optimizer. should be adam, sgd or adamw')\n",
        "\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5,\n",
        "                              cooldown=10, min_lr=1e-4, verbose=True)"
      ],
      "metadata": {
        "id": "5cCdULwxwHB2"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "def save_model(model, path):\n",
        "    torch.save(model.state_dict(), path)\n",
        "\n",
        "def load_model(model, path):\n",
        "    cuda = torch.cuda.is_available()\n",
        "    if cuda: device = 'cuda'\n",
        "    else: device = 'cpu'\n",
        "    model.load_state_dict(torch.load(path, map_location=device))"
      ],
      "metadata": {
        "id": "vsGIymLf5sPX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "best_valid_rate = 0\n",
        "num_epoch = rnn_config['num_epoch']\n",
        "print(f'begin training {num_epoch} epochs')\n",
        "model_names = []\n",
        "best_epoch = 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxL0WVO_6XUk",
        "outputId": "4687120d-ce9f-4351-bf6b-e75b6b8dd4a9"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "begin training 10 epochs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if best_epoch != 0:\n",
        "    load_model(model, out_dir + f'epoch_{best_epoch}.pt')\n",
        "\n",
        "\n",
        "for epoch in range(1, 1+num_epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for data, lengths in tqdm(dataloader, total=len(dataloader),\n",
        "                              desc=f'epoch {epoch}'): # batch_size = 512\n",
        "        # print(lengths)\n",
        "        lengths = [l-1 for l in lengths]\n",
        "        optimizer.zero_grad()\n",
        "        data = data.to(device)\n",
        "        preds = model(data, lengths)\n",
        "        targets = pack_padded_sequence(\n",
        "            data[:, 1:], lengths, batch_first=True, enforce_sorted=False).data\n",
        "        # preds: [num, len_alphabet-2], target: [num]\n",
        "        # print('hre pred, target, ', preds.shape, targets.shape)\n",
        "        loss = loss_function(preds, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "    train_losses.append(train_loss/train_size)\n",
        "    print(f'epoch {epoch}, train_loss: {train_losses[-1]:.3f}')\n",
        "    scheduler.step(train_losses[-1])\n",
        "    sampled_molecules = sample(model, vocab, batch_size=1024)\n",
        "    valid, unique = compute_valid_rate(sampled_molecules)\n",
        "    print(f'valid:  {(valid * 100):.3f} % | unique: {(unique * 100):.3f} %')\n",
        "    if valid > best_valid_rate:\n",
        "        best_epoch = epoch\n",
        "        trained_model_dir = out_dir + f'epoch_{epoch}.pt'\n",
        "        print(f'save model: ', trained_model_dir)\n",
        "        save_model(model, trained_model_dir)\n",
        "        model_names.append(trained_model_dir)\n",
        "        while len(model_names) > 10:\n",
        "            try:\n",
        "                remove_name = model_names.pop(0)\n",
        "                print('remove ', remove_name)\n",
        "                os.remove(remove_name)\n",
        "            except: print(f'error remove {model_names[0]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        },
        "id": "FyXA1vDIzH3E",
        "outputId": "7680e182-5656-4465-be2a-7d4189fffdda"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 1: 100%|██████████| 526/526 [02:28<00:00,  3.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1, train_loss: 43.709\n",
            "0\n",
            "valid:  100.000 % | unique: 0.000 %\n",
            "save model:  model_save/epoch_1.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 2: 100%|██████████| 526/526 [02:32<00:00,  3.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2, train_loss: 39.369\n",
            "0\n",
            "valid:  100.000 % | unique: 0.000 %\n",
            "save model:  model_save/epoch_2.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 3: 100%|██████████| 526/526 [02:27<00:00,  3.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3, train_loss: 36.672\n",
            "0\n",
            "valid:  100.000 % | unique: 0.000 %\n",
            "save model:  model_save/epoch_3.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 4:  20%|█▉        | 104/526 [00:29<01:59,  3.53it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-345f3715139d>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# print('hre pred, target, ', preds.shape, targets.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "compute_valid_rate(sampled_molecules)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YhIw1HzFy2V",
        "outputId": "a56b0242-986f-411b-8fb3-aebe0859fe03"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1024\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.0, 1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "qI6VWIDAGxSZ",
        "outputId": "e467a0ab-176b-458c-ca03-798dceafcf8f"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-3075b36c09c8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_mols"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "It5haeLQGFsw",
        "outputId": "6c6ba3dc-865e-4a22-86fc-a0a0f6c425c0"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "()"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_dir = out_dir + f'epoch_{best_epoch}.pt'\n",
        "with open(out_dir +'config.yaml', 'r') as f: config = yaml.full_load(f)\n",
        "model = RNN(config).to(device)\n",
        "load_model(model, best_dir)\n",
        "model.eval()\n",
        "num_batches = 10\n",
        "for _ in tqdm(range(num_batches)):\n",
        "    sampled_ints = model.sample(batch_size=batch_size,\n",
        "                                vocab=vocab, device=device).tolist()\n",
        "    mols = []\n",
        "    for ints in sampled_ints:\n",
        "        s_mol = []\n",
        "        for x in ints:\n",
        "            if vocab.int2token[x] =='<eos>':break\n",
        "            else: s_mol.append(vocab.int2token[x])\n",
        "        mols.append(\"\".join(s_mol))\n",
        "    mols = [sf.decoder(x) for x in mols]\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvVZumtW5rdI",
        "outputId": "6afce689-f32d-4192-a6d1-ffcf698176b4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "512"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selfie_vocab_path = '/content/Molecule-RNN/vocab/selfies_merged_vocab.yaml'\n",
        "with open(selfie_vocab_path, 'r') as f:\n",
        "    vocab = yaml.full_load(f)"
      ],
      "metadata": {
        "id": "HOP5-cKTc_Nd"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = SELFIEVocab(selfie_vocab_path)"
      ],
      "metadata": {
        "id": "tL0rshHldn80"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame([2, 3], columns=['SMILES'])\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "Dp5KfabVdowb",
        "outputId": "d39459fe-505f-4ea0-b5ed-e8cf6892f41f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   SMILES\n",
              "0       2\n",
              "1       3"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bfef732c-7ec8-4011-99b2-7e46dc8be1fd\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SMILES</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bfef732c-7ec8-4011-99b2-7e46dc8be1fd')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bfef732c-7ec8-4011-99b2-7e46dc8be1fd button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bfef732c-7ec8-4011-99b2-7e46dc8be1fd');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn([3, 4])\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQAAlK3Uok-Z",
        "outputId": "9cc68e27-bb68-4070-8ddb-d500c88c0b47"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1928,  1.4340, -0.1538,  1.4391],\n",
              "        [ 0.1171, -0.4854, -1.4899,  1.0256],\n",
              "        [ 1.6104,  0.7337, -0.1302, -1.3865]])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70ENYh4BrkKg",
        "outputId": "ba11a80b-7ccd-4384-e3e7-78592dab1c72"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0817, 0.4156, 0.0849, 0.4178],\n",
              "        [0.2365, 0.1295, 0.0474, 0.5866],\n",
              "        [0.6092, 0.2535, 0.1069, 0.0304]])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d = [x, x]\n",
        "torch.cat(d, -1).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V41BIApaon4d",
        "outputId": "27a19c91-fa9a-4e8e-e986-afba2c438e2d"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.view(x.shape[0], -1).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYfyHIi3o8Gh",
        "outputId": "e8cd2355-cbac-4ac3-dabb-d766c9094386"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_ = torch.multinomial(x.squeeze(), 1)\n",
        "x_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMqaBGVgf16z",
        "outputId": "f8e4a76b-6ea2-476b-8c8f-036de7d8198c"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[3],\n",
              "        [1],\n",
              "        [0]])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.ones([12, 1])*76"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jG7fKpasf5aR",
        "outputId": "413c09b5-6121-4799-94f3-b99e42793886"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[76.],\n",
              "        [76.],\n",
              "        [76.],\n",
              "        [76.],\n",
              "        [76.],\n",
              "        [76.],\n",
              "        [76.],\n",
              "        [76.],\n",
              "        [76.],\n",
              "        [76.],\n",
              "        [76.],\n",
              "        [76.]])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab.tokenizer_smiles(sfs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLPU8WzJjYe_",
        "outputId": "410045cd-2c05-48de-ca90-de19a3bf9c22"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[76, 14, 14, 63, 75]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab"
      ],
      "metadata": {
        "id": "vZGh0rZZjZtq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}